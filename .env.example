# -----------------------------------------------------------------------------
# Copy this file to ".env" and fill in your actual values
# -----------------------------------------------------------------------------

# Which backend to use: "lmstudio" or "litellm"
# If you set BACKEND=litellm, calls go to LiteLLM; otherwise LM Studio is used.
BACKEND=lmstudio

# URL of your LM Studio chat endpoint (OpenAI‐compatible)
LMSTUDIO_URL=http://127.0.0.1:1234/v1/chat/completions

# URL of your LiteLLM chat endpoint (OpenAI‐compatible)
LITELLM_URL=http://127.0.0.1:8080/v1/chat/completions

# Host and port for this FastAPI wrapper to bind to
WRAPPER_HOST=0.0.0.0
WRAPPER_PORT=8000

# DEBUG mode: "true" or "false"
DEBUG=false
