# -----------------------------------------------------------------------------
# Copy this file to ".env" and fill in your actual values.
# -----------------------------------------------------------------------------

# Which backend to use: "lmstudio" or "litellm"
BACKEND=litellm

# LM Studio chat endpoint (OpenAI-compatible)
LMSTUDIO_URL=http://127.0.0.1:1234/v1/chat/completions

# LiteLLM chat endpoint (OpenAI-compatible)
LITELLM_URL=http://127.0.0.1:8080/v1/chat/completions

# If LiteLLM requires an API key, set it here. Otherwise leave blank.
# By default, LiteLLM does NOT require a key. Only use this if you have configured
# LiteLLM (or a proxy) to enforce a Bearer token.
LITELLM_API_KEY=

# Host and port for this FastAPI wrapper to bind to
WRAPPER_HOST=0.0.0.0
WRAPPER_PORT=8000

# DEBUG mode: "true" or "false"
DEBUG=false
